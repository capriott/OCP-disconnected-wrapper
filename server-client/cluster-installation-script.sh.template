#!/bin/bash 

set -e

#==============================
echo "Starting Cluster Installation script"
#==============================
# Creating some variables
#==============================

homedir=/ec2-user

hostname=$(hostname)

CLUSTER_VERSION=$CLUSTER_VERSION

RELEASE_CHANNEL=$RELEASE_CHANNEL

export AWS_SHARED_CREDENTIALS_FILE=$homedir/.aws/credentials

#===============================================================
# Creating/Building imageset-config.yaml and install-config.yaml
#===============================================================

echo "Create the imageset-config.yaml file"

if ! [ -f $homedir/mirroring-workspace/imageset-config.yaml ]; then
cat <<EOF > "$homedir/mirroring-workspace/imageset-config.yaml"
apiVersion: mirror.openshift.io/v1alpha2
kind: ImageSetConfiguration
storageConfig:
  local:
    path: /home/ec2-user/mirroring-workspace/oc-mirror-metadata
mirror:
  platform:
    channels:
      - name: $RELEASE_CHANNEL
        minVersion: $CLUSTER_VERSION
        maxVersion: $CLUSTER_VERSION
EOF
else
  echo "The imageset-config.yaml file exists"
fi

#=================================
# Creating the install-config.yaml
#=================================

echo "Adding the CA to the install-config.yaml"

if ! grep -q 'additionalTrustBundle' $homedir/cluster/install-config.yaml; then
  echo "additionalTrustBundle:  |" >> $homedir/cluster/install-config.yaml
  echo "$(cat $homedir/registry-stuff/quay-rootCA/rootCA.pem)" | sed 's/^/      /' >> $homedir/cluster/install-config.yaml
else
  echo "The additionalTrustBundle already exists in the install-config"
fi

echo "Creating and adding the public-key to the install-config.yaml"

if ! [ -f $homedir/.ssh/cluster_key ]; then
  ssh-keygen -f $homedir/.ssh/cluster_key -t rsa -q -N ""
else
  echo "SSH key pair already exists"
fi

if ! grep -q 'sshKey' $homedir/cluster/install-config.yaml; then
  echo "sshKey: $(cat $homedir/.ssh/cluster_key.pub)" >> $homedir/cluster/install-config.yaml
else
  echo "SSH key already added to the install-config.yaml"
fi

echo "Adding pull-secret to the install-config.yaml"

if ! grep -q 'pullSecret' $homedir/cluster/install-config.yaml; then
  echo "pullSecret: '$(cat $homedir/.docker/config.json | jq -c )'" >> $homedir/cluster/install-config.yaml
else
  echo "The Pull Secret already exists in the install-config"
fi

echo "Cleanup the workspace"

rm -f $homedir/mirroring-workspace/oc-mirror.tar.gz
rm -f $homedir/pull-secret.template

#===========================================================================================================
# Downloading, unpacking installer, oc client and make changes to the manifest prior installing the cluster
#===========================================================================================================

echo "Starting Cluster deployment preparations"

echo "Creating the local binary /bin folder and exporting into PATH permanently"

if ! [ -d $homedir/bin ]; then
  mkdir $homedir/bin
  echo 'export PATH="/ec2-user/bin:$PATH"' >> $homedir/.bashrc
  source $homedir/.bashrc
  mv $homedir/mirroring-workspace/oc-mirror /ec2-user/bin
else
  echo "$homedir/bin directory already exists. Sourcing the .bashrc"
  source $homedir/.bashrc
fi

echo "Copying the pull-secret to home folder of container user"

if ! [ -d /home/ec2-user/.docker ]; then
  mkdir /home/ec2-user/.docker
  cp $homedir/.docker/config.json /home/ec2-user/.docker/config.json
else
  echo "The pull-secret to home folder of container user already exists"
fi

echo "Exporting the CA trust to be used by the container"

export SSL_CERT_FILE=$homedir/registry-stuff/quay-rootCA/rootCA.pem

cd $homedir/mirroring-workspace

echo "Mirroring release images for version $CLUSTER_VERSION"
oc-mirror --config $homedir/mirroring-workspace/imageset-config.yaml docker://$hostname:8443 --verbose 1

cd $homedir/cluster

echo "Downloading openshift-installer for version $CLUSTER_VERSION"

if ! [ -f $homedir/bin/openshift-install ]; then
  wget -q https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$CLUSTER_VERSION/openshift-install-linux.tar.gz

  echo "Unpacking openshift-installer"
  tar -xf openshift-install-linux.tar.gz
  rm -f openshift-install-linux.tar.gz
  mv ./openshift-install $homedir/bin
else
  echo "Openshift installer exists in the bin directory"
fi

echo "Downloading openshift-client"

if ! [ -f $homedir/bin/oc ]; then
  wget -q https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$CLUSTER_VERSION/openshift-client-linux.tar.gz

  echo "Unpacking openshift-client"
  tar -xf openshift-client-linux.tar.gz
  rm -f openshift-client-linux.tar.gz
  mv ./oc $homedir/bin
else
  echo "The openshift client exists in the bin directory"
fi

echo "Creating manifests"

cp install-config.yaml install-config.yaml.bak

if ! [ -d $homedir/cluster/manifests ]; then
  openshift-install create manifests --dir ./

  echo "Changing the DNS cluster manifest to avoid ingress operator trying to add '*.apps' domain"

  cd $homedir/cluster/manifests
  sed '/baseDomain:/q' cluster-dns-02-config.yml > new-cluster-dns-02-config.yml && mv new-cluster-dns-02-config.yml cluster-dns-02-config.yml
else
  echo "There is already a manifest directory present"
fi

#============================
# Launch cluster installation
#============================

echo "Launch cluster installation"
cd $homedir/cluster
openshift-install create cluster --dir ./ --log-level=info &

#=============================================
# Adding manually *apps. domain using aws CLI
#=============================================

echo "Waiting for the LB and the cluster zone to be created so to apply the wildcard '*.apps.' record"

while [ ! -f "$homedir/cluster/metadata.json" ]; do
  echo "Waiting for metadata.json to be available..."
  sleep 5  # Wait 5 seconds before checking again
done

region=$(jq -r '.aws.region' $homedir/cluster/metadata.json)
DOMAIN=$(jq -r '.aws.clusterDomain' $homedir/cluster/metadata.json)

# Checking if the zone is created for the cluster domain
while [[ -z $(aws route53 list-hosted-zones-by-name  --query "HostedZones[?Name=='$DOMAIN.'].Id" --output text) ]]; do
  echo "Zone is not ready yet"
  sleep 120
done

echo "Zone is ready. Waiting for the LB to be created"

# Maximum time in seconds to run the loop (20 minutes = 1200 seconds)
MAX_WAIT_TIME=1200

# Sleep interval between each iteration (in seconds)
SLEEP_INTERVAL=60

# Track total elapsed time
ELAPSED_TIME=0

# Get the cluster ID to compare with the LB VPC id so we find the correct LB to assosiate the apps. domain with.
INSTANCE_ID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
Cluster_VPC_id=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query 'Reservations[*].Instances[*].VpcId' --output text)
echo "The Cluster vpc ID is: $Cluster_VPC_id"

while [ $ELAPSED_TIME -lt $MAX_WAIT_TIME ]; do
    for i in {0..9}; do
        # Get the VPC ID for the current load balancer
        LB_VPC_id=$(aws elb describe-load-balancers --region ${region} --load-balancer-names | jq -r ".LoadBalancerDescriptions[$i].VPCId" 2>/dev/null)
        
        if [ -z "$LB_VPC_id" ]; then
            continue
        fi
        
        echo "Checking load balancer at index $i with VPC ID: $LB_VPC_id"

        # Compare the VPC ID
        if [ "$LB_VPC_id" == "$Cluster_VPC_id" ]; then
            echo "Match found for VPC ID: $LB_VPC_id at index $i. Adding the apps. record."

            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name  --query "HostedZones[?Name=='$DOMAIN.'].Id" --output text)
            ELB_ALIAS_TARGET=$(aws elb describe-load-balancers --region ${region} --load-balancer-names | jq -r ".LoadBalancerDescriptions[$i].CanonicalHostedZoneNameID")
            ELB_DNS_NAME=$(aws elb describe-load-balancers --region ${region} --load-balancer-names | jq -r ".LoadBalancerDescriptions[$i].DNSName")
            echo "The hosted zone ID is: $HOSTED_ZONE_ID"
            echo "The domain of the zone is: $DOMAIN"
            echo "The elb hosted zone ID is: $ELB_ALIAS_TARGET" 
            echo "The elb DNS name is: $ELB_DNS_NAME"
            
            aws route53 change-resource-record-sets \
            --hosted-zone-id "$HOSTED_ZONE_ID" \
            --change-batch '{
              "Changes": [
                {
                  "Action": "CREATE",
                  "ResourceRecordSet": {
                    "Name": "*.apps.'$DOMAIN'",
                    "Type": "A",
                    "AliasTarget": {
                      "HostedZoneId": "'$ELB_ALIAS_TARGET'",
                      "DNSName": "'$ELB_DNS_NAME'",
                      "EvaluateTargetHealth": false
                    }
                  }
                }
              ]
            }'
            
            # Exit the loop since we've found a match
            MATCH_FOUND=true
            break
        fi
    done

    if [ "$MATCH_FOUND" = true ]; then
      break
    fi

    # Increment the elapsed time
    ELAPSED_TIME=$((ELAPSED_TIME + SLEEP_INTERVAL))

    # Sleep before the next iteration
    echo "No match found, sleeping for $SLEEP_INTERVAL seconds..."
    sleep $SLEEP_INTERVAL
done

if [ "$MATCH_FOUND" = false ]; then
    echo "Timeout: No LB found to match the VPC id $ within 30 minutes."
    exit 1
fi

#============================================================
# Allow in node SG Groups access from the registry using SSH
#============================================================

echo "Allow SSH access from registry to the cluster nodes"

# Master-nodes

SG_MASTER_GROUP_NAME=$(jq -r '.cluster_id + "-master-sg"' terraform.tfvars.json)
echo "The SG name of master-sg is $SG_MASTER_GROUP_NAME"
SG_MASTER_GROUP_ID=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$Cluster_VPC_id --filters Name=tag:Name,Values=$SG_MASTER_GROUP_NAME | jq -r '.SecurityGroups[0].GroupId')
if [[  $SG_MASTER_GROUP_ID == "null" ]]; then
SG_MASTER_GROUP_NAME=$(jq -r '.cluster_id + "-controlplane"' terraform.tfvars.json);
echo "The SG name of controlplane is $SG_MASTER_GROUP_NAME";
fi
SG_MASTER_GROUP_ID=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$Cluster_VPC_id --filters Name=tag:Name,Values=$SG_MASTER_GROUP_NAME | jq -r '.SecurityGroups[0].GroupId')
echo "The SG ID for controlplane or master-sg is $SG_MASTER_GROUP_ID"
aws ec2 authorize-security-group-ingress --group-id $SG_MASTER_GROUP_ID --protocol tcp --port 22 --cidr 0.0.0.0/0

# Worker_nodes

SG_WORKER_GROUP_NAME=$(jq -r '.cluster_id + "-worker-sg"' terraform.tfvars.json)
echo "The SG name of worker-sg is $SG_WORKER_GROUP_NAME"
SG_WORKER_GROUP_ID=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$Cluster_VPC_id --filters Name=tag:Name,Values=$SG_WORKER_GROUP_NAME | jq -r '.SecurityGroups[0].GroupId')
if [[  $SG_WORKER_GROUP_ID == "null" ]]; then
SG_WORKER_GROUP_NAME=$(jq -r '.cluster_id + "-node"' terraform.tfvars.json);
echo "The SG name of node is $SG_WORKER_GROUP_NAME";
fi
SG_WORKER_GROUP_ID=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$Cluster_VPC_id --filters Name=tag:Name,Values=$SG_WORKER_GROUP_NAME | jq -r '.SecurityGroups[0].GroupId')
echo "The SG ID for node or worker-sg is $SG_WORKER_GROUP_ID"
aws ec2 authorize-security-group-ingress --group-id $SG_WORKER_GROUP_ID --protocol tcp --port 22 --cidr 0.0.0.0/0

#=================================
# Disable default Catalog Sources
#=================================

export KUBECONFIG=$homedir/cluster/auth/kubeconfig

oc patch OperatorHub cluster --type json \
    -p '[{"op": "add", "path": "/spec/disableAllDefaultSources", "value": true}]'